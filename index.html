<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>SVM Tutorial</title>
    <script type="module" crossorigin src="/assets/index-id3Iq8-g.js"></script>
    <link rel="stylesheet" crossorigin href="/assets/index-DFgMpK4Y.css">
  </head>

  <body>
    
      <h1>What is a Linear SVM?</h1>

      <p>Have you guys ever wondered how banks detect fraudulent transactions or how gmail classify which email is spam or not? Well, this is all
        because of the Support Vector Machines or SVMs which is a powerful algorithm that slices through complex data to uncover any hidden patterns. </p>

      <p>In simple terms, SVMs bascially create a decision boundary that allows machine to decide what option to pick. Lets go through how this works:</p>
      
      <p>Lets say we are looking at the different pitch type in baseball. We will be looking at 4 different pitch types: </p>
      <p>  -  Four-Seam Fastball (FF) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Cutter (FC) </p>
      <p>  - Changeup (CH)  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   - Slider (SL)  </p>

      <p>Given only the Release Speed and Release Spin Rate of the ball. Let's train a machine that has no knowledge of baseball and see what pitch type it will 
        predict based on only the Release Speed and Release Spin Rate </p>

      <p>This visualization down below represents the machine after it was trained with the baseball data and the decision boundary that was calculated by the SVM. 
        Feel free to check out all of the different Pitch Pairs and see where the decision boundary is drawn for each of the combination. 
      </p>

      <div id="app"></div>
      <div>

      <p> explain idk </p>

      <h2> The Math behind SVM</h2>

      <p>The idea behind maximum margin classifers are to implement linear separability to correctly classify data into categories.<br>
        Support Vector Machines are a form of margin margin classifers that aim to have large margins between the decision boundary and training point.<br>
      </p>
      <p>For SVMs, a support vector is a training point such that \(y_i \vec{w} \cdot \text{Aug}(\vec{x}^{(i)}) = 1\)<br>
        Solutions for Hard-SVMs aim to have perfect classification with no slack; therefore, the solution for the problem becomes \(\vec{w}^\star\ = \sum_{i \in S} y_i a_i \text{Aug}(\vec{x}^{(i)})\)</p>
      <p>Alternatively, we the soft-SVM problem allows for some classifications to be wrong with distance \(\epsilon_i \)<br>
        The problem becomes: \(\min_{\vec(w) \in \mathbb{R}^{d+1}, \vec{\epsilon} \in \mathbb{R}^n} \lVert{\vec{w}}\rVert^2 + C \sum_{i=1}^n \epsilon_i \)<br>
        Where, \(C\) is considered the slack parameter: Large \(C\) avoids misclassifications with low slack and Small \(C\) allows for more slack at the cost of misclassifications.
      </p>

    </div>
  </body>
</html>